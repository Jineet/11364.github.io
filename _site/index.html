<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>11-364</title>

    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <meta name="viewport" content="width=device-width">
    <script src="https://use.fontawesome.com/d92a8f6a37.js"></script>
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>11-364 Introduction to Human Aided Deep Learning</h1>
        <p>These notes accompany 11-364, the undergraduate course in Deep Learning @ Carnegie Mellon.</p>
        <p>Professor: James K. Baker</p>


      </header>
      <section>

        <h1 id="header-1-about"><a href="#header-1"></a> About</h1>

<p>11-364 is a hands-on, project-based reading and research course.  There will be lectures on core material, hands-on tutorials, individual and team presentations, other class discussions, and a major team project.</p>

<h1 id="header-1-schedule"><a href="#header-1"></a> Schedule</h1>



        <div style="padding-bottom:20px;">
            <h2 >Introduction to 11-364 </h2>
            <p style="display:inline;"> If you are excited about deep learning or about this opportunity to be at the cutting edge, please take this course.</p>

        <a style="padding-right: 10px;padding-left: 15px;display:inline;" href="files/presentations/introduction_to_11364/introduction_to_11364.pdf" download>
          <i class="fa fa-file-pdf-o" aria-hidden="true"></i> pdf
        </a>
        <a href="files/presentations/introduction_to_11364/introduction_to_11364.pptx" download>
          <i class="fa fa-file-powerpoint-o" aria-hidden="true"></i> pptx
        </a>
      </div>




        <div style="padding-bottom:20px;">
            <h2 >Deep Neural Networks & Backpropagation </h2>
            <p style="display:inline;">  Topics include information about course, computing with a network of simple units, computation of feedforward activation, minibatches, the backpropagation algorithm, stochastic gradient descent, and iterative training.</p>

        <a style="padding-right: 10px;padding-left: 15px;display:inline;" href="files/presentations/deep_neural_networks_and_backpropagation/deep_neural_networks_and_backpropagation.pdf" download>
          <i class="fa fa-file-pdf-o" aria-hidden="true"></i> pdf
        </a>
        <a href="files/presentations/deep_neural_networks_and_backpropagation/deep_neural_networks_and_backpropagation.pptx" download>
          <i class="fa fa-file-powerpoint-o" aria-hidden="true"></i> pptx
        </a>
      </div>




        <div style="padding-bottom:20px;">
            <h2 >Improving Neural Network Training (Pt. 1) </h2>
            <p style="display:inline;"> Holdout and cross-validation, cross-entropy cost function, softmax, log-likelihood cost function, L2 and L1 regularization, dropout, artificial expansion of training data, initializing weights, heuristics to set hyperparameters, and other optimizations.</p>

        <a style="padding-right: 10px;padding-left: 15px;display:inline;" href="files/presentations/improving_neural_network_training_part_1/improving_neural_network_training_part_1.pdf" download>
          <i class="fa fa-file-pdf-o" aria-hidden="true"></i> pdf
        </a>
        <a href="files/presentations/improving_neural_network_training_part_1/improving_neural_network_training_part_1.pptx" download>
          <i class="fa fa-file-powerpoint-o" aria-hidden="true"></i> pptx
        </a>
      </div>




        <div style="padding-bottom:20px;">
            <h2 >Improving Neural Network Training (Pt. 2) </h2>
            <p style="display:inline;"> Artificial expansion of training data, design decisions and hyperparameters, final layer activation choice, experimentation advice, setting the learning rate, momentum, ReLU, stopping early, regularization parameters, mini-batch optimization, and Nesterov accelerated momentum.</p>

        <a style="padding-right: 10px;padding-left: 15px;display:inline;" href="files/presentations/improving_neural_network_training_part_2/improving_neural_network_training_part_2.pdf" download>
          <i class="fa fa-file-pdf-o" aria-hidden="true"></i> pdf
        </a>
        <a href="files/presentations/improving_neural_network_training_part_2/improving_neural_network_training_part_2.pptx" download>
          <i class="fa fa-file-powerpoint-o" aria-hidden="true"></i> pptx
        </a>
      </div>




        <div style="padding-bottom:20px;">
            <h2 >Momentum and ReLU </h2>
            <p style="display:inline;"> A discussion of momentum, Nesterov accelerated momentum and the Rectified Linear Unit (ReLU).</p>

        <a style="padding-right: 10px;padding-left: 15px;display:inline;" href="files/presentations/momentum_relu/momentum_relu.pdf" download>
          <i class="fa fa-file-pdf-o" aria-hidden="true"></i> pdf
        </a>
        <a href="files/presentations/momentum_relu/momentum_relu.pptx" download>
          <i class="fa fa-file-powerpoint-o" aria-hidden="true"></i> pptx
        </a>
      </div>

      <div style="padding-bottom:20px;">
          <h2 >Distilling Knowledge in a Neural Network </h2>
          <p style="display:inline;"> A discussion of a paper on Knowledge distillitation by Geoff Hinton, Google. This paper focusses on a technique to improve neural network performance at scale by distilling the knowledge in an ensemble of models into a single model</p>

      <a href="files/presentations/distilling_knowledge_in_a_neural_network/Distilling_Knowledge_in_a_Neural_Network.pptx" download>
        <i class="fa fa-file-powerpoint-o" aria-hidden="true"></i> pptx
      </a>
    </div>







      </section>
      <footer>

      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>



  </body>
</html>
